{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Introduction\n","\n","The world of data science and machine learning is constantly evolving, with new techniques, breaking news and technologies emerging every day. One way to stay up-to-date with these trends is to engage with online communities such as Reddit. In this notebook, we'll use the Reddit API to collect data from three of the largest and most active subreddits focused on data science, machine learning, and artificial intelligence: `r/datascience`, `r/machinelearning`, and `r/artificial` and apply natural language processing techniques to extract insights from the top posts and their comments over the past few years. Through this project, we aim to gain a better understanding of current trends and sentiments in the data science community.  \n","By doing this, we hope to shed light on the attitudes and opinions of the community towards various topics. This project will focus on top posts and comments from the last few years."]},{"cell_type":"markdown","metadata":{},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:08:40.727825Z","iopub.status.busy":"2023-04-07T19:08:40.727129Z","iopub.status.idle":"2023-04-07T19:08:54.886530Z","shell.execute_reply":"2023-04-07T19:08:54.885203Z","shell.execute_reply.started":"2023-04-07T19:08:40.727744Z"},"trusted":true},"outputs":[],"source":["%pip install pygwalker praw"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:08:54.890315Z","iopub.status.busy":"2023-04-07T19:08:54.889052Z","iopub.status.idle":"2023-04-07T19:09:09.216942Z","shell.execute_reply":"2023-04-07T19:09:09.215687Z","shell.execute_reply.started":"2023-04-07T19:08:54.890259Z"},"trusted":true},"outputs":[],"source":["# Import libraries\n","import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pygwalker as pyg\n","from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n","\n","from transformers import pipeline\n","from datasets import Dataset\n","\n","import praw"]},{"cell_type":"markdown","metadata":{},"source":["To be able to use this notebook its needed to create a Reddit app and get the credentials specified below. We recommend to store those crendentials in a .env file and load them using the dotenv package or, like in this case, to use the kaggle_secrets package.  \n","If you want to reuse the generated CSV files, you won't need to set up your Reddit credentials since these files will be loaded automatically in the following cells."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.219536Z","iopub.status.busy":"2023-04-07T19:09:09.218591Z","iopub.status.idle":"2023-04-07T19:09:09.756415Z","shell.execute_reply":"2023-04-07T19:09:09.755249Z","shell.execute_reply.started":"2023-04-07T19:09:09.219485Z"},"trusted":true},"outputs":[],"source":["# Import to use Kaggle secrets\n","from kaggle_secrets import UserSecretsClient\n","\n","user_secrets = UserSecretsClient()\n","client_id = user_secrets.get_secret(\"CLIENT_ID\")\n","client_secret = user_secrets.get_secret(\"CLIENT_SECRET\")\n","redirect_uri = user_secrets.get_secret(\"REDIRECT_URI\")\n","user_agent = user_secrets.get_secret(\"USER_AGENT\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.759612Z","iopub.status.busy":"2023-04-07T19:09:09.759225Z","iopub.status.idle":"2023-04-07T19:09:09.766593Z","shell.execute_reply":"2023-04-07T19:09:09.765342Z","shell.execute_reply.started":"2023-04-07T19:09:09.759572Z"},"trusted":true},"outputs":[],"source":["# Setup matplotlib integration\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.768689Z","iopub.status.busy":"2023-04-07T19:09:09.768098Z","iopub.status.idle":"2023-04-07T19:09:09.842219Z","shell.execute_reply":"2023-04-07T19:09:09.841184Z","shell.execute_reply.started":"2023-04-07T19:09:09.768652Z"},"trusted":true},"outputs":[],"source":["# Create Reddit client instance\n","reddit = praw.Reddit(\n","    client_id=client_id,\n","    client_secret=client_secret,\n","    redirect_uri=redirect_uri,\n","    user_agent=user_agent,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Data collection\n","\n","To collect the data we use the PRAW package, which is a Python wrapper for the Reddit API. To learn more about this package check the [documentation](https://praw.readthedocs.io/en/latest/)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.844273Z","iopub.status.busy":"2023-04-07T19:09:09.843890Z","iopub.status.idle":"2023-04-07T19:09:09.853510Z","shell.execute_reply":"2023-04-07T19:09:09.852199Z","shell.execute_reply.started":"2023-04-07T19:09:09.844233Z"},"trusted":true},"outputs":[],"source":["def get_top_posts(subreddit_list=\"MachineLearning\", limit=1_000, time_filter=\"all\"):\n","    \"\"\"\n","    Get top posts from a list of subreddits.\n","    \"\"\"\n","\n","    # initialize posts dataframe\n","    posts_df = []\n","\n","    # get top posts from subreddit list\n","    posts = reddit.subreddit(subreddit_list).top(time_filter=time_filter, limit=limit)\n","\n","    # loop through posts and append to dataframe\n","    for post in posts:\n","        posts_df.append(\n","            {\n","                \"post_id\": post.id,\n","                \"subreddit\": post.subreddit,\n","                \"author\": post.author,\n","                \"created_utc\": post.created_utc,\n","                \"post_url\": post.url,\n","                \"post_title\": post.title,\n","                \"link_flair_text\": post.link_flair_text,\n","                \"score\": post.score,\n","                \"num_comments\": post.num_comments,\n","                \"upvote_ratio\": post.upvote_ratio,\n","            }\n","        )\n","\n","    return pd.DataFrame(posts_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.856471Z","iopub.status.busy":"2023-04-07T19:09:09.855826Z","iopub.status.idle":"2023-04-07T19:09:09.866379Z","shell.execute_reply":"2023-04-07T19:09:09.865047Z","shell.execute_reply.started":"2023-04-07T19:09:09.856429Z"},"trusted":true},"outputs":[],"source":["def get_comments(post_ids, limit=None):\n","    \"\"\"\n","    Get comments from a list of post ids.\n","    \"\"\"\n","\n","    # initialize comments dataframe\n","    comments_df = []\n","\n","    for post_id in post_ids:\n","        # get comments from post\n","        submission = reddit.submission(id=post_id)\n","        submission.comments.replace_more(limit=limit)\n","        comments = submission.comments.list()\n","\n","        # append comments to dataframe\n","        for comment in comments:\n","            comments_df.append(\n","                {\n","                    \"author\": comment.author,\n","                    \"post_id\": post_id,\n","                    \"body\": comment.body,\n","                    \"distinguished\": comment.distinguished,\n","                    \"is_submitter\": comment.is_submitter,\n","                    \"score\": comment.score,\n","                    \"created_utc\": comment.created_utc,\n","                }\n","            )\n","\n","    return pd.DataFrame(comments_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.870936Z","iopub.status.busy":"2023-04-07T19:09:09.869651Z","iopub.status.idle":"2023-04-07T19:09:09.883176Z","shell.execute_reply":"2023-04-07T19:09:09.882136Z","shell.execute_reply.started":"2023-04-07T19:09:09.870883Z"},"trusted":true},"outputs":[],"source":["# Output paths\n","posts_csv_path = \"/kaggle/working/DS_ML_AI_posts.csv\"\n","comments_csv_path = \"/kaggle/working/DS_ML_AI_comments.csv\""]},{"cell_type":"markdown","metadata":{},"source":["While the download of the data related to the posts does not suppose a significan cost in time, the download of the comments is a very time consuming process. If you want to download the data again just delete the CSV files and rerun the notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.885719Z","iopub.status.busy":"2023-04-07T19:09:09.884568Z","iopub.status.idle":"2023-04-07T19:09:09.917982Z","shell.execute_reply":"2023-04-07T19:09:09.916935Z","shell.execute_reply.started":"2023-04-07T19:09:09.885673Z"},"trusted":true},"outputs":[],"source":["# Load posts csv file if it exists, if not get top posts and save to csv\n","if os.path.exists(posts_csv_path):\n","    posts_df = pd.read_csv(posts_csv_path)\n","else:\n","    posts_df = get_top_posts(\n","        subreddit_list=\"MachineLearning+datascience+artificial\",\n","        limit=3_000,\n","        time_filter=\"all\",\n","    )\n","    posts_df.to_csv(posts_csv_path, index=False, header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:09.923333Z","iopub.status.busy":"2023-04-07T19:09:09.922799Z","iopub.status.idle":"2023-04-07T19:09:10.985645Z","shell.execute_reply":"2023-04-07T19:09:10.984326Z","shell.execute_reply.started":"2023-04-07T19:09:09.923301Z"},"trusted":true},"outputs":[],"source":["# Load comments csv file if it exists, if not fetch comments and save to csv\n","if os.path.exists(comments_csv_path):\n","    comments_df = pd.read_csv(comments_csv_path)\n","else:\n","    comments_df = get_comments(\n","        post_ids=posts_df[\"post_id\"].values,\n","        limit=None,\n","    )\n","    comments_df.to_csv(comments_csv_path, index=False, header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:10.987884Z","iopub.status.busy":"2023-04-07T19:09:10.987418Z","iopub.status.idle":"2023-04-07T19:09:10.997269Z","shell.execute_reply":"2023-04-07T19:09:10.996194Z","shell.execute_reply.started":"2023-04-07T19:09:10.987839Z"},"trusted":true},"outputs":[],"source":["posts_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:10.999473Z","iopub.status.busy":"2023-04-07T19:09:10.998707Z","iopub.status.idle":"2023-04-07T19:09:11.019495Z","shell.execute_reply":"2023-04-07T19:09:11.018476Z","shell.execute_reply.started":"2023-04-07T19:09:10.999435Z"},"trusted":true},"outputs":[],"source":["comments_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.021681Z","iopub.status.busy":"2023-04-07T19:09:11.021220Z","iopub.status.idle":"2023-04-07T19:09:11.044818Z","shell.execute_reply":"2023-04-07T19:09:11.043918Z","shell.execute_reply.started":"2023-04-07T19:09:11.021643Z"},"trusted":true},"outputs":[],"source":["posts_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.046941Z","iopub.status.busy":"2023-04-07T19:09:11.046555Z","iopub.status.idle":"2023-04-07T19:09:11.060683Z","shell.execute_reply":"2023-04-07T19:09:11.059373Z","shell.execute_reply.started":"2023-04-07T19:09:11.046904Z"},"trusted":true},"outputs":[],"source":["comments_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.062961Z","iopub.status.busy":"2023-04-07T19:09:11.062292Z","iopub.status.idle":"2023-04-07T19:09:11.081179Z","shell.execute_reply":"2023-04-07T19:09:11.079889Z","shell.execute_reply.started":"2023-04-07T19:09:11.062925Z"},"trusted":true},"outputs":[],"source":["# Number of posts by subreddit\n","posts_df.groupby(\"subreddit\")[\"post_id\"].count()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.083208Z","iopub.status.busy":"2023-04-07T19:09:11.082795Z","iopub.status.idle":"2023-04-07T19:09:11.338779Z","shell.execute_reply":"2023-04-07T19:09:11.337708Z","shell.execute_reply.started":"2023-04-07T19:09:11.083168Z"},"trusted":true},"outputs":[],"source":["# Number of comments by subreddit\n","posts_df.merge(comments_df, on=\"post_id\").groupby(\"subreddit\")[\"body\"].count()"]},{"cell_type":"markdown","metadata":{},"source":["# Data processing\n","\n","The data processing step for this project is relatively simple, as we can leverage the rich data source provided by the Reddit API. However, we will still need to perform some preprocessing steps to clean up the data. For example, we will:\n","- Remove columns that we will not be using in the analysis.\n","- Clean up comments made by bots, such as those made by u/RemindMeBot, or comments that don't have a body.\n","- Add the subreddit to the comments dataframe.\n","- Create a new year column by parsing the created_utc column to a datetime object, which will allow us to analyze trends over time."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.341162Z","iopub.status.busy":"2023-04-07T19:09:11.340675Z","iopub.status.idle":"2023-04-07T19:09:11.362141Z","shell.execute_reply":"2023-04-07T19:09:11.361010Z","shell.execute_reply.started":"2023-04-07T19:09:11.341104Z"},"trusted":true},"outputs":[],"source":["# Convert created_utc to datetime on posts dataframe\n","posts_df[\"created_utc\"] = pd.to_datetime(posts_df[\"created_utc\"], unit=\"s\")\n","posts_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.364227Z","iopub.status.busy":"2023-04-07T19:09:11.363865Z","iopub.status.idle":"2023-04-07T19:09:11.385957Z","shell.execute_reply":"2023-04-07T19:09:11.384848Z","shell.execute_reply.started":"2023-04-07T19:09:11.364189Z"},"trusted":true},"outputs":[],"source":["# Convert created_utc to datetime on comments dataframe\n","comments_df[\"created_utc\"] = pd.to_datetime(comments_df[\"created_utc\"], unit=\"s\")\n","comments_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.388349Z","iopub.status.busy":"2023-04-07T19:09:11.387708Z","iopub.status.idle":"2023-04-07T19:09:11.406992Z","shell.execute_reply":"2023-04-07T19:09:11.405890Z","shell.execute_reply.started":"2023-04-07T19:09:11.388300Z"},"trusted":true},"outputs":[],"source":["# Create year column on posts dataframe\n","posts_df[\"year\"] = posts_df[\"created_utc\"].dt.year\n","posts_df[\"year\"].astype(int)\n","posts_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.408905Z","iopub.status.busy":"2023-04-07T19:09:11.408532Z","iopub.status.idle":"2023-04-07T19:09:11.445521Z","shell.execute_reply":"2023-04-07T19:09:11.444231Z","shell.execute_reply.started":"2023-04-07T19:09:11.408868Z"},"trusted":true},"outputs":[],"source":["# Create year column on comments dataframe\n","comments_df[\"year\"] = comments_df[\"created_utc\"].dt.year\n","comments_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.447663Z","iopub.status.busy":"2023-04-07T19:09:11.447266Z","iopub.status.idle":"2023-04-07T19:09:11.455049Z","shell.execute_reply":"2023-04-07T19:09:11.453645Z","shell.execute_reply.started":"2023-04-07T19:09:11.447624Z"},"trusted":true},"outputs":[],"source":["# Remove columns that are not needed for the analysis from posts dataframe\n","posts_df = posts_df.drop(columns=[\"post_url\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.456911Z","iopub.status.busy":"2023-04-07T19:09:11.456454Z","iopub.status.idle":"2023-04-07T19:09:11.483050Z","shell.execute_reply":"2023-04-07T19:09:11.482100Z","shell.execute_reply.started":"2023-04-07T19:09:11.456870Z"},"trusted":true},"outputs":[],"source":["# Remove columns that are not needed for the analysis from the comments dataframe\n","comments_df = comments_df.drop(columns=[\"distinguished\", \"is_submitter\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.484962Z","iopub.status.busy":"2023-04-07T19:09:11.484565Z","iopub.status.idle":"2023-04-07T19:09:11.495593Z","shell.execute_reply":"2023-04-07T19:09:11.494053Z","shell.execute_reply.started":"2023-04-07T19:09:11.484925Z"},"trusted":true},"outputs":[],"source":["# Check rows with nan values in posts dataframe\n","posts_df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.497956Z","iopub.status.busy":"2023-04-07T19:09:11.497563Z","iopub.status.idle":"2023-04-07T19:09:11.561380Z","shell.execute_reply":"2023-04-07T19:09:11.559881Z","shell.execute_reply.started":"2023-04-07T19:09:11.497919Z"},"trusted":true},"outputs":[],"source":["# Check rows with nan values in comments dataframe\n","comments_df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.562952Z","iopub.status.busy":"2023-04-07T19:09:11.562608Z","iopub.status.idle":"2023-04-07T19:09:11.636119Z","shell.execute_reply":"2023-04-07T19:09:11.634726Z","shell.execute_reply.started":"2023-04-07T19:09:11.562924Z"},"trusted":true},"outputs":[],"source":["# Remove comments with no body\n","comments_df.dropna(inplace=True, subset=[\"body\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.638133Z","iopub.status.busy":"2023-04-07T19:09:11.637488Z","iopub.status.idle":"2023-04-07T19:09:11.664699Z","shell.execute_reply":"2023-04-07T19:09:11.663799Z","shell.execute_reply.started":"2023-04-07T19:09:11.638093Z"},"trusted":true},"outputs":[],"source":["# Convert comment body to string type\n","comments_df[\"body\"] = comments_df[\"body\"].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.666971Z","iopub.status.busy":"2023-04-07T19:09:11.666392Z","iopub.status.idle":"2023-04-07T19:09:11.673019Z","shell.execute_reply":"2023-04-07T19:09:11.671702Z","shell.execute_reply.started":"2023-04-07T19:09:11.666933Z"},"trusted":true},"outputs":[],"source":["# Convert post title to string type\n","posts_df[\"post_title\"] = posts_df[\"post_title\"].astype(str)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.681469Z","iopub.status.busy":"2023-04-07T19:09:11.681188Z","iopub.status.idle":"2023-04-07T19:09:11.687796Z","shell.execute_reply":"2023-04-07T19:09:11.686721Z","shell.execute_reply.started":"2023-04-07T19:09:11.681444Z"},"trusted":true},"outputs":[],"source":["# Convert link_flair_text to Categorical type in posts dataframe\n","posts_df[\"link_flair_text\"] = posts_df[\"link_flair_text\"].astype('category')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.691068Z","iopub.status.busy":"2023-04-07T19:09:11.689728Z","iopub.status.idle":"2023-04-07T19:09:11.958661Z","shell.execute_reply":"2023-04-07T19:09:11.957598Z","shell.execute_reply.started":"2023-04-07T19:09:11.691035Z"},"trusted":true},"outputs":[],"source":["# Remove u/RemindMeBot related comments\n","comments_df.drop(comments_df[comments_df.author == \"RemindMeBot\"].index, inplace=True)\n","comments_df.drop(comments_df[comments_df[\"body\"].str.contains(\"RemindMe!\")].index, inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:11.960534Z","iopub.status.busy":"2023-04-07T19:09:11.960101Z","iopub.status.idle":"2023-04-07T19:09:12.049990Z","shell.execute_reply":"2023-04-07T19:09:12.049042Z","shell.execute_reply.started":"2023-04-07T19:09:11.960481Z"},"trusted":true},"outputs":[],"source":["# Add subreddit to comments dataframe\n","comments_df = comments_df.merge(posts_df[['post_id', 'subreddit']], on='post_id', how='left')\n","comments_df.head()"]},{"cell_type":"markdown","metadata":{},"source":["# EDA\n","\n","The EDA (Exploratory Data Analysis) part in this notebook is relatively straightforward. The main goal is to gain a general understanding of the data and identify any trends or patterns within the data. To make this process even more straightforward, we have installed and imported `pygwalk`. To use this package, simply uncomment the code below. This package allows us to create interactive plots and explore the data in a more dynamic way."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:12.053618Z","iopub.status.busy":"2023-04-07T19:09:12.052616Z","iopub.status.idle":"2023-04-07T19:09:12.058436Z","shell.execute_reply":"2023-04-07T19:09:12.057378Z","shell.execute_reply.started":"2023-04-07T19:09:12.053577Z"},"trusted":true},"outputs":[],"source":["# Use pygwalker to explore the data and get insights more easily\n","# gwalker = pyg.walk(posts_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:12.061035Z","iopub.status.busy":"2023-04-07T19:09:12.059741Z","iopub.status.idle":"2023-04-07T19:09:12.074002Z","shell.execute_reply":"2023-04-07T19:09:12.072929Z","shell.execute_reply.started":"2023-04-07T19:09:12.060984Z"},"trusted":true},"outputs":[],"source":["# Set sns style for the following plots\n","sns.set_style(\"whitegrid\")"]},{"cell_type":"markdown","metadata":{},"source":["In the next plot we can see the number of posts per year in the three subreddits and how the number of posts has increased over the years. But it's also worth noting that the posts being analyzed in this notebook are only the top posts of each subreddit so the following analysis is not representative of the whole community or the real activity in the subreddits. We'll also take a look at the number of comments."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:12.076261Z","iopub.status.busy":"2023-04-07T19:09:12.075724Z","iopub.status.idle":"2023-04-07T19:09:12.532619Z","shell.execute_reply":"2023-04-07T19:09:12.530129Z","shell.execute_reply.started":"2023-04-07T19:09:12.076153Z"},"trusted":true},"outputs":[],"source":["# Number of posts by year on each subreddit\n","plt.figure(figsize=(12, 8))\n","\n","sns.countplot(\n","    x=\"year\",\n","    hue=\"subreddit\",\n","    data=posts_df,\n","    palette=\"Set2\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:12.534889Z","iopub.status.busy":"2023-04-07T19:09:12.534223Z","iopub.status.idle":"2023-04-07T19:09:13.106877Z","shell.execute_reply":"2023-04-07T19:09:13.105768Z","shell.execute_reply.started":"2023-04-07T19:09:12.534846Z"},"trusted":true},"outputs":[],"source":["# Number of comments per Year for each subreddit\n","plt.figure(figsize=(12, 8))\n","\n","sns.countplot(\n","    x=\"year\",\n","    hue=\"subreddit\",\n","    data=comments_df,\n","    palette=\"Set2\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:13.109117Z","iopub.status.busy":"2023-04-07T19:09:13.108385Z","iopub.status.idle":"2023-04-07T19:09:13.402759Z","shell.execute_reply":"2023-04-07T19:09:13.401801Z","shell.execute_reply.started":"2023-04-07T19:09:13.109074Z"},"trusted":true},"outputs":[],"source":["# Average score per post\n","plt.figure(figsize=(12, 8))\n","\n","sns.boxplot(\n","    x=\"subreddit\",\n","    y=\"score\",\n","    data=posts_df,\n","    palette=\"Set2\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:13.405399Z","iopub.status.busy":"2023-04-07T19:09:13.404019Z","iopub.status.idle":"2023-04-07T19:09:13.961596Z","shell.execute_reply":"2023-04-07T19:09:13.957476Z","shell.execute_reply.started":"2023-04-07T19:09:13.405352Z"},"trusted":true},"outputs":[],"source":["# Number of posts by flair\n","fig = plt.figure(figsize=(12, 8))\n","\n","sns.countplot(\n","    y=\"link_flair_text\",\n","    data=posts_df,\n","    palette=\"Set2\",\n","    order=posts_df[\"link_flair_text\"].value_counts().index,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:13.967549Z","iopub.status.busy":"2023-04-07T19:09:13.964982Z","iopub.status.idle":"2023-04-07T19:09:15.399175Z","shell.execute_reply":"2023-04-07T19:09:15.398027Z","shell.execute_reply.started":"2023-04-07T19:09:13.967467Z"},"trusted":true},"outputs":[],"source":["# Check trending terms on post titles using wordcloud\n","plt.figure(figsize=(12, 8))\n","\n","posts_title_text = \" \".join([title for title in posts_df[\"post_title\"].str.lower()])\n","\n","wcloud = WordCloud(\n","    collocation_threshold=2, width=960, height=540, background_color=\"white\"\n",").generate(posts_title_text)\n","\n","plt.axis(\"off\")\n","plt.imshow(wcloud)\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["By analyzing the number of posts and comments per year for each subreddit from 2013 to the first quarted of 2023, we can see that these communities on Reddit have experienced significant growth over the past years. The `r/datascience` subreddit seems to have become the most active community with a substantial increase in activity since 2020. In comparison, the levels of activity on `r/machinelearning` and `r/artificial` has remained relatively stable over the same period although both have also experienced clear growth.  \n","These findings suggest that the data science community on Reddit is rapidly expanding, with an increasing number of individuals seeking out information and resources related to data science and machine learning. This growth may reflect a broader trend towards the democratization of data science, as more people become interested in and involved in the field."]},{"cell_type":"markdown","metadata":{},"source":["# Sentiment analysis\n","\n","To perform the sentiment analysis, we have decided to use the roBERTa-based model [cardiffnlp/twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest) from the [Huggingface](https://huggingface.co/) library. This model has been pre-trained on a large corpus of Twitter data and fine-tuned for sentiment analysis, making it well-suited for our purposes.  \n","The labels provided by the model are:\n","- `0` for positive.\n","- `1` for neutral\n","- `2` for positive.\n","\n","It is important to note that the model has certain limitations and may not provide 100% accurate results. Since the model was trained on tweets, it may not perform as well on Reddit comments due to the differences in tone and language. The model may also struggle with identifying sarcasm or other forms of nuanced language commonly used on Reddit. However, despite these limitations, the analysis can still offer valuable insights into the overall sentiment of comments in the data science subreddits."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:15.400775Z","iopub.status.busy":"2023-04-07T19:09:15.400358Z","iopub.status.idle":"2023-04-07T19:09:29.920506Z","shell.execute_reply":"2023-04-07T19:09:29.919471Z","shell.execute_reply.started":"2023-04-07T19:09:15.400731Z"},"trusted":true},"outputs":[],"source":["MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n","pipe = pipeline(\"sentiment-analysis\", model=MODEL, tokenizer=MODEL, max_length=512, truncation=True, device=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:29.922900Z","iopub.status.busy":"2023-04-07T19:09:29.922102Z","iopub.status.idle":"2023-04-07T19:09:31.538130Z","shell.execute_reply":"2023-04-07T19:09:31.536739Z","shell.execute_reply.started":"2023-04-07T19:09:29.922858Z"},"trusted":true},"outputs":[],"source":["# Test the model\n","pipe(\"I love this! ðŸ’•\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:31.540224Z","iopub.status.busy":"2023-04-07T19:09:31.539792Z","iopub.status.idle":"2023-04-07T19:09:31.563278Z","shell.execute_reply":"2023-04-07T19:09:31.562016Z","shell.execute_reply.started":"2023-04-07T19:09:31.540184Z"},"trusted":true},"outputs":[],"source":["pipe(\"I hate this! ðŸ¤®\")"]},{"cell_type":"markdown","metadata":{},"source":["# Sentiment analysis of comments related to ChatGPT\n","\n","In this section we'll analyze the sentiment expressed in comments related to ChatGPT, an advanced language model developer by OpenAI. To do this, we'll analyze the distribution of sentiments in comments that contain keywords related to ChatGPT."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:31.565378Z","iopub.status.busy":"2023-04-07T19:09:31.564776Z","iopub.status.idle":"2023-04-07T19:09:34.268292Z","shell.execute_reply":"2023-04-07T19:09:34.267019Z","shell.execute_reply.started":"2023-04-07T19:09:31.565336Z"},"trusted":true},"outputs":[],"source":["# Create dataframe with GPT related comments\n","gpt_comments_df = comments_df[\n","    comments_df[\"body\"].str.contains(\n","        \"chat gpt|chatgpt|gpt\", regex=True, case=False,\n","    )\n","]\n","\n","gpt_comments_df = gpt_comments_df.reset_index(drop=True)\n","gpt_comments_df.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:34.270510Z","iopub.status.busy":"2023-04-07T19:09:34.269870Z","iopub.status.idle":"2023-04-07T19:09:34.288463Z","shell.execute_reply":"2023-04-07T19:09:34.287017Z","shell.execute_reply.started":"2023-04-07T19:09:34.270469Z"},"trusted":true},"outputs":[],"source":["gpt_comments_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:34.290393Z","iopub.status.busy":"2023-04-07T19:09:34.290012Z","iopub.status.idle":"2023-04-07T19:09:34.317577Z","shell.execute_reply":"2023-04-07T19:09:34.316405Z","shell.execute_reply.started":"2023-04-07T19:09:34.290356Z"},"trusted":true},"outputs":[],"source":["# Create dataset with gpt_comments_df's body column\n","gpt_comments_dataset = Dataset.from_pandas(gpt_comments_df[[\"body\"]])\n","gpt_comments_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:09:34.319600Z","iopub.status.busy":"2023-04-07T19:09:34.319240Z","iopub.status.idle":"2023-04-07T19:10:07.188020Z","shell.execute_reply":"2023-04-07T19:10:07.186822Z","shell.execute_reply.started":"2023-04-07T19:09:34.319563Z"},"trusted":true},"outputs":[],"source":["# Make sentiment analysis of ChatGPT related comments\n","results = pipe(gpt_comments_dataset[\"body\"])\n","results[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:07.189928Z","iopub.status.busy":"2023-04-07T19:10:07.189420Z","iopub.status.idle":"2023-04-07T19:10:07.200733Z","shell.execute_reply":"2023-04-07T19:10:07.199295Z","shell.execute_reply.started":"2023-04-07T19:10:07.189865Z"},"trusted":true},"outputs":[],"source":["# Convert sentiment analysis results into a pd.Series\n","sentiment_series = pd.Series([result[\"label\"] for result in results])\n","sentiment_series"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:07.203153Z","iopub.status.busy":"2023-04-07T19:10:07.202388Z","iopub.status.idle":"2023-04-07T19:10:07.233840Z","shell.execute_reply":"2023-04-07T19:10:07.232970Z","shell.execute_reply.started":"2023-04-07T19:10:07.203113Z"},"trusted":true},"outputs":[],"source":["# Concat gpt_comments_df with sentiments analysis results\n","gpt_comments_df = pd.concat([gpt_comments_df, sentiment_series], axis=1).rename(columns={0: 'sentiment'})\n","gpt_comments_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:07.235639Z","iopub.status.busy":"2023-04-07T19:10:07.235220Z","iopub.status.idle":"2023-04-07T19:10:07.247495Z","shell.execute_reply":"2023-04-07T19:10:07.246570Z","shell.execute_reply.started":"2023-04-07T19:10:07.235594Z"},"trusted":true},"outputs":[],"source":["# Number of comments for each sentiment category\n","gpt_comments_df[\"sentiment\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:07.249610Z","iopub.status.busy":"2023-04-07T19:10:07.249074Z","iopub.status.idle":"2023-04-07T19:10:07.513011Z","shell.execute_reply":"2023-04-07T19:10:07.511876Z","shell.execute_reply.started":"2023-04-07T19:10:07.249422Z"},"trusted":true},"outputs":[],"source":["# Sentiments distribution in ChatGPT related comments\n","plt.figure(figsize=(12, 8))\n","\n","sns.countplot(\n","    x=\"sentiment\",\n","    data=gpt_comments_df,\n","    palette=\"Set2\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:07.515500Z","iopub.status.busy":"2023-04-07T19:10:07.514592Z","iopub.status.idle":"2023-04-07T19:10:07.829352Z","shell.execute_reply":"2023-04-07T19:10:07.828406Z","shell.execute_reply.started":"2023-04-07T19:10:07.515452Z"},"trusted":true},"outputs":[],"source":["# Sentiments distribution in ChatGPT related comments by subreddit\n","plt.figure(figsize=(12, 8))\n","\n","sns.countplot(\n","    x=\"subreddit\",\n","    hue=\"sentiment\",\n","    data=gpt_comments_df,\n","    palette=\"Set2\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["According to the sentiment analysis results, the majority of comments related to ChatGPT are neutral, indicating that users express balanced opinions about this technology. However, a significant number of negative comments also exist, suggesting areas of concern within the community regarding its potential impact on areas such as the workplace, personal development, and the emergence or loss of jobs"]},{"cell_type":"markdown","metadata":{},"source":["# Sentiment Overview"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:07.831474Z","iopub.status.busy":"2023-04-07T19:10:07.830762Z","iopub.status.idle":"2023-04-07T19:10:07.998929Z","shell.execute_reply":"2023-04-07T19:10:07.997975Z","shell.execute_reply.started":"2023-04-07T19:10:07.831419Z"},"trusted":true},"outputs":[],"source":["# Create dataset with comments_df body column\n","comments_dataset = Dataset.from_pandas(comments_df[[\"body\"]])\n","comments_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:10:08.001516Z","iopub.status.busy":"2023-04-07T19:10:08.000524Z","iopub.status.idle":"2023-04-07T19:54:07.300534Z","shell.execute_reply":"2023-04-07T19:54:07.299531Z","shell.execute_reply.started":"2023-04-07T19:10:08.001472Z"},"trusted":true},"outputs":[],"source":["# Make sentiment analysis\n","results = pipe(comments_dataset[\"body\"])\n","results[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:54:07.302750Z","iopub.status.busy":"2023-04-07T19:54:07.302262Z","iopub.status.idle":"2023-04-07T19:54:07.370829Z","shell.execute_reply":"2023-04-07T19:54:07.368062Z","shell.execute_reply.started":"2023-04-07T19:54:07.302713Z"},"trusted":true},"outputs":[],"source":["# Convert sentiment analysis results into a pd.Series\n","sentiment_series = pd.Series([result[\"label\"] for result in results])\n","sentiment_series"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:54:07.374355Z","iopub.status.busy":"2023-04-07T19:54:07.373942Z","iopub.status.idle":"2023-04-07T19:54:07.563753Z","shell.execute_reply":"2023-04-07T19:54:07.562723Z","shell.execute_reply.started":"2023-04-07T19:54:07.374311Z"},"trusted":true},"outputs":[],"source":["# Concat comments_df with sentiments analysis results\n","comments_df = pd.concat([comments_df, sentiment_series], axis=1).rename(columns={0: 'sentiment'})\n","comments_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:54:07.565822Z","iopub.status.busy":"2023-04-07T19:54:07.565135Z","iopub.status.idle":"2023-04-07T19:54:10.068530Z","shell.execute_reply":"2023-04-07T19:54:10.066913Z","shell.execute_reply.started":"2023-04-07T19:54:07.565770Z"},"trusted":true},"outputs":[],"source":["# Save comments with sentiment analysis results into CSV\n","comments_with_sentiment_csv_path = \"/kaggle/working/DS_ML_AI_comments_with_sentiments.csv\"\n","comments_df.to_csv(comments_with_sentiment_csv_path, index=False, header=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:54:10.073036Z","iopub.status.busy":"2023-04-07T19:54:10.072631Z","iopub.status.idle":"2023-04-07T19:54:10.095524Z","shell.execute_reply":"2023-04-07T19:54:10.094542Z","shell.execute_reply.started":"2023-04-07T19:54:10.073000Z"},"trusted":true},"outputs":[],"source":["# Number of comments for each sentiment category\n","comments_df[\"sentiment\"].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-07T19:54:10.097771Z","iopub.status.busy":"2023-04-07T19:54:10.097349Z","iopub.status.idle":"2023-04-07T19:54:10.500900Z","shell.execute_reply":"2023-04-07T19:54:10.499845Z","shell.execute_reply.started":"2023-04-07T19:54:10.097728Z"},"trusted":true},"outputs":[],"source":["# Sentiments distribution in comments\n","plt.figure(figsize=(12, 8))\n","\n","sns.countplot(\n","    x=\"sentiment\",\n","    data=comments_df,\n","    palette=\"Set2\",\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Following the sentiment analysis performed on the ChatGPT related comments, we performed a sentiment analysis on all comments in the dataset. The results reveal that, similar to the previous analysis, the majority of comments are neutral followed by negative comments and lastly, positive ones. This finding suggests that users tend to express fairly balanced views on the topics discussed in these comments, albeit with a certain level of dissatisfaction or criticism."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
